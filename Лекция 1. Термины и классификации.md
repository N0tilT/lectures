[[Параллельные вычисления]]

Методы и способы повышения производительности
1. Создать ресурс параллелизма - разбить задачу для распараллеливания
2. Управление ресурсом параллелизма
3. Сбор данных

Оценка эффективности
Эффективность можно померить
- Количество операций в секунду - на одном наборе данных
- Оперативная память

Есть задачи, которые нельзя параллелить
- Сильно связанные данные

Требования
- Независимость между устрйоствами вычислительной машины (особенно ввод-вывод)
- Избыточность элементов вычислительной системы

Организация параллельных вычислений
- Массивно-параллельная система - несколько узлов, включающих 1 или несколько процессоров и локальную память. Узлы связаны через некоторую коммутационную среду
- Кластерная система - набор ПК общего назначения, связанных через стандартные сетевые технологии
- Grid (вычислительная система) - объединение разнородных вычислительных ресурсов в сетях. Большие задержки в передаче данных
- Параллельные векторные системы PVP. Несколько процессоров, могут быть связаны коммутатором
- Симметричные мультипроцессорные системы SMP - несколько однородных процессоров с общей памятью
- Система с неоднородным доступом к памяти NUMA - 


Классификация параллельных архитектур
 - SISD - одиночный поток команд и одиночный поток данных
 - SIMD - одиночный поток команд и множественный поток данных
 - MISD - множественный поток команд и одиночный поток данных
 - MIMD - множественный поток команд - множественный поток данных

CUDA - параллелизм по данным

### Типы парраллелизма
##### Параллелизм по данным
Параллелизм по данным связан с разделением данных на подмножества, которые могут обрабатываться одновременно. При этом одна и та же операция применяется к различным частям данных
##### Параллелизм задач
Параллелизм задач предполагает разделение различных задач или процессов, которые могут выполняться одновременно. В этом случае разные задачи могут не зависеть друг от друга